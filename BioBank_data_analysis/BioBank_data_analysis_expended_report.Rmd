---
title: "UKBioBank data exploration"
author: "Nitay Alon"
date: "December 16, 2018"
output: html_document
---

In this report we explore the Human Brain Data from UK-Bio Bank as a preliminary step of applying our mixture of sexes model and hypothesis testing. We begin with extraction of relevant data for our research, as described in the 15/11/2018 meeting summary. we the proceed to visual inspection of relevant demographic data and begin analysis.

# EDA
```{r loading relevant libraries and data,message=FALSE,warning=FALSE,include=FALSE}
library(dplyr)
library(knitr)
library(tidyr)
library(magrittr)
library(ggplot2)
library(Jmisc)
library(e1071)
sourceAll("../BioBank_data_analysis/Data_preparation_methods/")
sourceAll("../BioBank_data_analysis/Visualization_methods/")
sourceAll("../BioBank_data_analysis/Hypothesis_testing_methods/")
source("../Mixture_models/source_script.R")
# Load the BioBank data:
bio.bank.data <- read.csv("../Data/Biobank/ukb24562.csv", 
                          header = T, stringsAsFactors = F)

relevant_region <- c(seq(45,92)
                     ,seq(141,167)
                     ,seq(93,140)
                     ,seq(177,194)
                     ,seq(195,333))
relevant_data <- bio.bank.data[,relevant_region]
# Adding sex,DOB,Ethnicity
demographic_data <- bio.bank.data[,c(2,3,seq(13,15))]
names(demographic_data) <- c("Sex","Year_Of_Birth",
                             "Ethnic_BG_1",
                             "Ethnic_BG_2",
                             "Ethnic_BG_3")
# Join
full_relevant_data <- cbind(demographic_data, relevant_data)
```

```{r exploring sex and ethnicity, echo=TRUE,warning=FALSE,include=FALSE}
# Removing missing data
# sapply(full_relevant_data, function(x){sum(is.na(x))})

# Some statistics on the demographic data 
full_relevant_data$Sex %>% table() %>% prop.table()
full_relevant_data$Ethnic_BG_1 %>% table()
```
And we can see that sex wise our data is balanced, however, from ethnic point of view we have high bias toward white population.

## Kolmogorov Smirnov test

Next we apply K-S test for each column to determine if the data is Normally or Log-Normally distributed.
We begin with testing for normal distribution.
```{r applying KS Test over features}
ks_results_log_normal_data <- applyKSTestForDataFrame(relevant_data,"L")
ks_results_normal_data <- applyKSTestForDataFrame(relevant_data,"N")
```

Now, we can compute the number of features for which the KS test wasn't rejected:
```{r computing number of non significant features}
table(ks_results_log_normal_data$P_Value > 0.05)
summary(ks_results_log_normal_data$P_Value)
hist(ks_results_log_normal_data$P_Value)
hist(ks_results_log_normal_data$KS_Statistics)
summary(ks_results_log_normal_data$KS_Statistics)

table(ks_results_normal_data$P_Value > 0.05)
summary(ks_results_normal_data$P_Value)
hist(ks_results_normal_data$KS_Statistics)
```

the results above indicate that indeed the log-normal model is more likely than the normal model,however this is only a preliminary testing.
Moreover, the testing above assumes a single distribution model, while we assume mixture model,thus it might be beneficial to split the data according to the sex and apply KS test per gender data.

```{r separating the feature data to genders and apply ks test}

```

## Validating the log-normal transformations
To validate the log transformation we examine if the third and froth moment of the logged data follow the expected behavior of the moments of normal distribution - 
\[
E(X^3) = 0 \\
E(X^4) = 3
\]
```{r validate 3rd and 4th moment}
data_for_log_and_scaled <- sapply(relevant_data, prepareDataForLogTransformation)
data_for_log_and_scaled %<>% data.frame()
logged_and_scaled <- sapply(data_for_log_and_scaled, function(x)
  {
  scale(log(x))
  })
logged_and_scaled %<>% data.frame()
moments_validation <- sapply(logged_and_scaled, validateMoments)
moments_validation %<>% t()

hist(moments_validation[,1],breaks = 100, main = "Histogram of 3rd moment")
hist(moments_validation[,2],breaks = 100, main = "Histogram of 4th moment")
```

Here we can see that some features do not follow the log-normal assumption. We can drill down and see some examples of features that violates our model:
```{r log normal violating features}
extreme_feature_name_fourth_moment <- names(which.max(moments_validation[,2]))
extreme_feature_name_third_moment <- names(which.max(moments_validation[,1]))

hist(logged_and_scaled$X25069.2.0, breaks = 100)
summary(logged_and_scaled$X25069.2.0)
head(sort(logged_and_scaled$X25069.2.0, decreasing = F),10)

hist(logged_and_scaled$X25147.2.0, breaks = 100)
summary(logged_and_scaled$X25147.2.0)

quantile(logged_and_scaled$X25147.2.0, 0.02, na.rm = T)
sum(logged_and_scaled$X25147.2.0 > 6, na.rm = T)
head(sort(logged_and_scaled$X25147.2.0, decreasing = T),5)
prop.table(table(full_relevant_data$Sex[logged_and_scaled$X25147.2.0 >= 6]))
prop.table(table(full_relevant_data$Sex))
```

here we see that in the first case the problem can be addressed by outlier removal but in the second case it is not the case.

### Deeper dive to skewed distribution analysis
We take a deeper look in the distributions that show high skewness:
```{r Kurtosis features}
head(rownames(moments_validation)[order(moments_validation[,2],decreasing = T)],5)

hist(logged_and_scaled$X25069.2.0, breaks = 100)
summary(logged_and_scaled$X25069.2.0)

hist(logged_and_scaled$X25068.2.0, breaks = 100)
summary(logged_and_scaled$X25068.2.0)

hist(logged_and_scaled$X25075.2.0, breaks = 100)
summary(logged_and_scaled$X25075.2.0)

hist(logged_and_scaled$X25077.2.0, breaks = 100)
summary(logged_and_scaled$X25077.2.0)

hist(logged_and_scaled$X25122.2.0, breaks = 100)
summary(logged_and_scaled$X25122.2.0)
```

And it seems that those distributions are indeed centered, but they have one or two outliers.

```{r skewed features}
head(rownames(moments_validation)[order(moments_validation[,1],decreasing = T)],5)

hist(logged_and_scaled$X25147.2.0, breaks = 100)
summary(logged_and_scaled$X25147.2.0)

hist(logged_and_scaled$X25146.2.0, breaks = 100)
summary(logged_and_scaled$X25146.2.0)

hist(logged_and_scaled$X25115.2.0, breaks = 100)
summary(logged_and_scaled$X25115.2.0)

hist(logged_and_scaled$X25131.2.0, breaks = 100)
summary(logged_and_scaled$X25131.2.0)

hist(logged_and_scaled$X25130.2.0, breaks = 100)
summary(logged_and_scaled$X25130.2.0)
```

## Composite vs simple hypothesis testing
First we test the single distribution vs composite distribution hypotheses:
\[
H_0:f_X(x) = f_X(x) \\
H_1:f_X(x) = pf_X(x) + (1-p)g_X(x)
\]

```{r composite hypothesis log-normal assumption,warning=FALSE,include=FALSE}
hypothesis_result <- 
sapply(relevant_data, function(x){
  simpleDistributionVsCompositeDistribution(x,
full_relevant_data$Sex,"L")
})
```

One feature breaks the EM method:
```{r debugging problematic feature}
df <- data.frame("X25069.2.0" = full_relevant_data$X25069.2.0,
                 "Sex" = full_relevant_data$Sex)
bioBankHistogramBySex(df, "X25069.2.0")
```
Here we see that the extreme observations in the men data cause the EM to explode. This particular feature worth separate investigation.

### Summarizng the first hypothesis test

In order to decide for which features we reject the null hypothesis we compare the $llrt$ with the critical value of the proper $\chi^2$ statistic. Using Wald's theorem we know that under the null hypothesis the llrt is distributed $\chi^2$ with $5-2$ df (We have 2 parameters in the null hypothesis and 5 in the alternative).
After FDR correction we use the 99\% percentile of this distribution:
```{r computing the rejection region}
critical_value <- qchisq(0.99,3)
hypothesis_result %<>% as.data.frame()
names(hypothesis_result) <- c("llrt")
hypothesis_result$reject_the_null_hypothesis <- 
  hypothesis_result$llrt > critical_value
hypothesis_result$reject_the_null_hypothesis %>% 
  table() %>% 
  prop.table()
```
And we can see that $93\%$ of the features are assumed to be a mixture model and not pure, single distribution.

# Mixture model - pure type vs Mixture of Mixtures
Our main focus is on testing our mixture of mixture model, to test these hypotheses we apply the composite hypothesis log-likelihood ratio test.

## Log normal assumption
We start with assuming log-normal model
```{r compute llrt for mixture models assuming log-normal ,warning=FALSE,include=FALSE}
composite_hypothesis_result <- 
sapply(relevant_data, function(x){
  pureTypeMixtureVsCompositeMixture(x,
full_relevant_data$Sex,"L")
})
```

Now, We plot the llrt results:
```{r plotting llrt results}
hist(composite_hypothesis_result, breaks = 50)
abline(v = qexp(0.99,0.5), col = "red")
```
(red line indicating the $99\%$ percentile of $exp(\frac{1}{2})$).

### Analyzing low llrt
We see that some features have llrt less than 1 - let's review them separately.
```{r validating weird llrt, warning=FALSE,include=FALSE}
composite_hypothesis_result %<>% data.frame()
names(composite_hypothesis_result) <- "llrt"

composite_hypothesis_result$feature_id <- 
  rownames(composite_hypothesis_result)
```

First we notice that one feature has Nan llrt:
```{r Nan llrt}
X25069.2.0 <- na.omit(relevant_data$X25069.2.0)
X25069.2.0 %>% prepareDataForLogTransformation() %>% log() %>% summary()
hist(X25069.2.0, breaks = 100)
```

by looking at the histogram we see that most of the data is centered at 0.7 but there's a left tail:

```{r left tail of Nan llrt}
summary(X25069.2.0)
head(sort(X25069.2.0,decreasing = F),5)
quantile(X25069.2.0,c(0,0.01,0.05))
```
so one observation $0$ causes the problem. If we repeat the hypothesis testing and remove this observation:
```{r repeating the EM zero removed}
gender_data_for_X25069.2.0 <- 
  full_relevant_data$Sex[!is.na(relevant_data$X25069.2.0)]

full_em_for_X25069.2.0 <- pureTypeMixtureVsCompositeMixture(X25069.2.0[X25069.2.0>0],
                                  gender_data = gender_data_for_X25069.2.0,"L",return_full_data = T)
```
Plotting the distribution:
```{r plotting the extreme distribution}
data_for_ggplot_X25069.2.0 = data.frame(value = X25069.2.0,
                                        bio_sex = gender_data_for_X25069.2.0)

ggplot(data_for_ggplot_X25069.2.0, aes(value, fill = factor(bio_sex))) + 
  geom_histogram(bins = 100, aes(y = (..density..)))
```
and this model seems more like the "normal and extreme population model".

```{r validating weird llrt log normal data, warning=FALSE,include=FALSE}
less_than_zero_llrt_features_names <- 
    composite_hypothesis_result$feature_id[composite_hypothesis_result$llrt < 0]

less_than_zero_llrt_features_col_location <- 
  names(relevant_data) %in% 
  less_than_zero_llrt_features_names

data_for_composite_hypothesis_testing_small_llrt <- 
  relevant_data[,less_than_zero_llrt_features_col_location]

composite_hypothesis_less_than_zero_llrt_em <- 
lapply(data_for_composite_hypothesis_testing_small_llrt, 
       function(x){
  pureTypeMixtureVsCompositeMixture(x,
full_relevant_data$Sex,"L",T)
})            
```

Selecting the smallest value:
```{r analyzing low llrt,warning=FALSE,include=FALSE}
llrt_less_than_0 <- 
  lapply(composite_hypothesis_less_than_zero_llrt_em, function(x)x$llrt)

composite_hypothesis_less_than_zero_llrt_em[[which.min(llrt_less_than_0)]]$alternative_hypothesis$m_parameters

composite_hypothesis_less_than_zero_llrt_em[[which.min(llrt_less_than_0)]]$null_hypothesis
```

Plotting the distribution:

```{r low llrt log normal data}
X25826.2.0 <- na.omit(relevant_data$X25826.2.0)
X25826.2.0 %>% 
  prepareDataForLogTransformation() %>% 
  log() %>% 
  summary()

gender_data_for_X25826.2.0 <- 
  full_relevant_data$Sex[!is.na(relevant_data$X25826.2.0)]

data_for_ggplot_X25826.2.0 = data.frame(value = X25826.2.0 %>% 
  prepareDataForLogTransformation() %>% 
  log(),
                                        bio_sex = gender_data_for_X25826.2.0)

ggplot(data_for_ggplot_X25826.2.0, aes(value, fill = factor(bio_sex))) + 
  geom_histogram(bins = 100, aes(y = (..density..)))
```

And we can see a clear single distribution. 

### Analysis of composite hypothesis testing:

In order to determine the number of significant features we use Wald's theorem again this time with $\chi^2_2$ distribution.
```{r number of significant features composite hypothesis}
critical_value <- qexp(0.99,1/2)
composite_hypothesis_result %<>% as.data.frame()
names(composite_hypothesis_result) <- c("llrt")
composite_hypothesis_result$reject_the_null_hypothesis <- 
  composite_hypothesis_result$llrt > critical_value
composite_hypothesis_result$reject_the_null_hypothesis %>% 
  table() %>% 
  prop.table()
```
So, for the small majority of the features we reject the null hypothesis.
Next we repeat the analysis assuming normal distribution.

# Normal distribution assumption analysis
Next we repeat the analysis, but assuming normal distribution instead of log-normal.

## Simple vs composite
```{r composite hypothesis normal assumption,warning=FALSE,include=FALSE}
simple_hypothesis_result_normal_assumption <- 
sapply(relevant_data, function(x){
  simpleDistributionVsCompositeDistribution(x,
full_relevant_data$Sex,"N")
})
```

### Summarizng the simple vs composite hypothesis

As before, we use the same critical value to reject the null hypothesis.

```{r computing the rejection region normal assumption}
critical_value <- qchisq(0.99,3)
simple_hypothesis_result_normal_assumption %<>% as.data.frame()
names(simple_hypothesis_result_normal_assumption) <- c("llrt")
simple_hypothesis_result_normal_assumption$reject_the_null_hypothesis <- 
  simple_hypothesis_result_normal_assumption$llrt > critical_value
simple_hypothesis_result_normal_assumption$reject_the_null_hypothesis %>%
  table() %>% 
  prop.table()
```
And we can see that almost $90\%$ of the features are assumed to be a mixture model and not pure, single distribution.

## Pure type mixture model vs mixture of mixture
As before, we now test the hypotheses that the data was generated by a mixture model of 2 pure types (null) vs the alternative, that the data is generated by a mixture of mixtures.
```{r removing non significant features, warning=FALSE,include=FALSE}
composite_features_normal <- rownames(simple_hypothesis_result_normal_assumption[simple_hypothesis_result_normal_assumption$reject_the_null_hypothesis,])

which_features_are_sig_normal <- names(relevant_data) %in% composite_features_normal

data_for_composite_hypothesis_testing_normal_assumption <- 
  relevant_data[,which_features_are_sig_normal]
```

using EM we can compute the llrt:
Now we apply the composite hypothesis log-likelihood ratio test:
```{r compute llrt for mixture models under normal assumption,warning=FALSE,include=FALSE}
composite_hypothesis_result_normal_assumption <- 
sapply(data_for_composite_hypothesis_testing_normal_assumption, function(x){
  pureTypeMixtureVsCompositeMixture(x,
full_relevant_data$Sex,"N")
})
```
As before, we begin with visual inquiry:
Now, We plot the llrt results:
```{r plotting llrt results for normal data}
hist(composite_hypothesis_result_normal_assumption, 
     col = rgb(1,0,1,alpha = 0.3), breaks = 50,
     main = "Histogram of llrt for normal data")
```
And we can compare the performance of the log-normal assumption vs the normal assumption:
```{r comparing normal to log normal assumption}
hist(composite_hypothesis_result_normal_assumption, 
     col = rgb(1,0,1,alpha = 0.3), breaks = 50,
     main = "Histogram of llrt for normal data")
hist(composite_hypothesis_result[,1], 
     col = rgb(1,1,0,alpha = 0.3),
     breaks = 50, add = T)
legend("topright", c("Normal", "Log normal"),
       col=c("Pink", "Orange"), lwd=10)
```

Analysis of llrt:
```{r number of significant features composite hypothesis normal daat assumption}
critical_value <- qexp(0.99,1/2)
composite_hypothesis_result_normal_assumption %<>% as.data.frame()
names(composite_hypothesis_result_normal_assumption) <- c("llrt")
composite_hypothesis_result_normal_assumption$reject_the_null_hypothesis <- 
  composite_hypothesis_result_normal_assumption$llrt > critical_value
composite_hypothesis_result_normal_assumption$reject_the_null_hypothesis %>% 
  table() %>% 
  prop.table()
```
And we see that the normal assumption yields poor results when compared to the log-normal data.

# Summary

