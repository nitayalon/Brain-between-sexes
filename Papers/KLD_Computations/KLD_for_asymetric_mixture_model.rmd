---
title: "KLD estimation for asymetric mixture model"
author: "Nitay Alon"
date: "8/9/2018"
output: html_document
---

In this paper I'll introduce the idea of using Kullback-Leibler divergence (KLD) as a way to estimate a $50\%$ power region for log-likelihood ratio test (llrt) to test the hypothesis of pure types vs mixture types over the parameter set. 

In the first section I'll explore the mathematical aspects of the KLD and explain the estimation method. In the second section I'll present some interesting findings and explain how we can use them as guidelines to test the research's main hypothesis.
Conclusions and future work are discussed in the final section.

## KLD as an estimator of $50\%$ power region
KLD can be explained as the expectation of the log likelihood ratio between two probabilistic models with respect to one of the models:
\[
KLD = \int log(\frac{f_1(x)}{f_0(x)})f_1(x)dx
\]
where $f_1(x)$ denotes the alternative hypothesis and $f_0(x)$ denotes the null hypothesis. In the context of our research they are the mixture model and the pure types respectively. 
We can think of this size as a way to measure the "distance" between both models, *i.e* when the alternative hypothesis is the correct model than the KLD grows, while if the null model is the right model KLD will be close to zero.

### Estimating KLD using Monte-Carlo
Since we've described the KLD as the expected value (mean) of the log likelihood ratio we can use MC simulation to estimate it. First we sample observations from the mixture model (alternative hypothesis). Next we compute the llr for each observation by computing the likelihood under the alternative and under the null hypothesis to produce:
\[
\frac{f_1(x)}{f_0(x)}
\]
Applying log and summing the likelihood ratios yields the log-likelihood ratio:
\[
\sum_{i=1}^{n} log(\frac{f_1(x_i)}{f_0(x_i)})
\]
Next we simply compute the mean - 
\[
\hat{KLD} = \frac{\sum_{i=1}^{n} log(\frac{f_1(x_i)}{f_0(x_i)})}{n}
\]

### Motivation to use KLD
In this part, we'll describe the motivation to use KLD as a way to partition the parameters set in to "potential discovery region" and other.
When we test the hypothesis of pure types vs mixture model we can use the log-likelihood ratio test (llrt). According to Wilks' theorem, under the null hypothesis the asymptotic distribution of this ratio is $\chi^2_{df}$:
\[
\lambda = log(\frac{f_1(x)}{f_0(x)}) \sim \chi^2_d
\]
where $d$ is the difference between the number of parameters in the alternative model and the null model, In our case $d=2$. Since we know that $\chi^2_2 = \frac{1}{2}exp(1) \implies exp(2)$ we can set a rejection region based on the quantiles of the exponential distribution. Since we're correcting for multiple comparisments we can set a rejection level at about 2.7.
Next we can what is the probability of seeing a log-likelihood ratio of 2.7 if the alternative hypothesis is true, we can show that the KLD is a lower bound to this size. Using the KLD we can partition the parameter space to "potential discovery" subset. 

## Results
```{r Loading_the_environment, echo=FALSE}
source("~/mastersdegree/Thesis/DAPHNA_JOEL/Mixture_models/KLD_Computation/generate_sample_from_mixture_model.R")
source("~/mastersdegree/Thesis/DAPHNA_JOEL/Mixture_models/KLD_Computation/KLD_computation.R")
source("~/mastersdegree/Thesis/DAPHNA_JOEL/Mixture_models/KLD_Computation/main_function.R")
source("~/mastersdegree/Thesis/DAPHNA_JOEL/Mixture_models/KLD_Computation/validate_sample_means_and_variance.R")

# results <- mainFunction(sample_size = 1e3,create_auto_grid = T)
# write.csv(results,"~/mastersdegree/Thesis/DAPHNA_JOEL/Results/KLD/KLD_computation_10_08_2018.csv")
KLD <- read.csv("~/mastersdegree/Thesis/DAPHNA_JOEL/Results/KLD/KLD_computation_10_08_2018.csv")
```
### Valdaiting the estimation process:
Since we've used a MC to estimate KLD let's validate the simulation. We know that the population should have a mean 0 and variance of 1.
```{r validating population mean, echo = TRUE}
mean(KLD$pop_mean)
sd(KLD$pop_mean)
hist(KLD$pop_mean, freq = F, breaks = 100, main = "Histogram of population mean")
```
And repeating the process with the population variance:
```{r validating population variance, echo = TRUE}
mean(KLD$pop_var)
sd(KLD$pop_var)
hist(KLD$pop_var, freq = F, breaks = 100, main = "Histogram of population variance")
lines(density(rchisq(1e5,90000) / (90000 - 1)), col = "red")
cut(KLD$pop_var,c(0,0.8,1,1.2,2)) %>% 
  table() %>% 
  prop.table() 
range(KLD$pop_var)
```
Let's analyze the population variance.
```{r analyzing the population variance, echo=TRUE}
which.max(KLD$pop_var)
max_var_data <- subset(KLD, KLD$pop_var == max(KLD$pop_var), c(xi, epsilon,delta,male_var))
with(max_var_data, male_var + (xi + epsilon)*(xi + delta))
```
and we can see that the numeric solution yields the expected population variance of 1.

### Analyzing the KLD estimation results
In this section I'll present an analysis of the KLD computation as described above.
We'll begin with a simple histogram of the estimated KLD (the 2.5 line is marked with vertical red line). 
```{r plotting KLD, echo = TRUE}
range(KLD$KLD_pop)
hist(KLD$KLD_pop, freq = F, breaks = 100, main = "Histogram of KLD estimation")
abline(v = 2.5, col = "red")
```
As we can see in the following table that about 17.6% of the parameters set lays in the "discovery region" 
```{r prop table of KLD, echo=TRUE}
library(dplyr)
cut(KLD$KLD_pop, breaks = c(0,2.5,Inf)) %>% 
  table() %>% 
  prop.table()
```
Next we plot $\xi$ vs KLD:
```{r KLD vs xi, echo=TRUE}
with(KLD, plot(xi, KLD_pop))
```
Next we create a surface plot of KLD as a function of $\xi, \varepsilon, \delta$.
```{r surface plot of KLD}
library(plotly)
library(reshape)

xi <- 0.1
dat <- subset(KLD, xi == 0.1, c(epsilon,delta,KLD_pop))
point_plot_KLD <- plot_ly(x = ~dat$epsilon, y = ~dat$delta, z = ~dat$KLD_pop) %>% add_markers()
point_plot_KLD
```


```{r KLD ~ epsilon + delta, echo=TRUE}
point_plot_KLD <- plot_ly(x = ~KLD$epsilon, y = ~KLD$delta, z = ~KLD$KLD_pop) %>% add_markers()
point_plot_KLD
```

Next, we'd like to see the maximal value of KLD as a function of  $\xi$
```{r maximum KLD ~ xi, echo=TRUE}
KLD_xi <- plot_ly(x = ~KLD$xi, y = ~KLD$epsilon, z = ~KLD$KLD_pop) %>% add_markers()
KLD_xi

KLD_means <- plot_ly(x = ~(KLD$xi + KLD$epsilon), 
                     y = ~(-KLD$xi - KLD$delta), 
                     z = ~KLD$KLD_pop) %>% 
  add_markers()
KLD_means
```
